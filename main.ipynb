{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『2022 CCF BDCI』- 基于TrustAI的阅读理解可解释性评测基线\n",
    "## 1、项目介绍\n",
    "深度学习模型在很多NLP任务上已经取得巨大成功，但其常被当作一个黑盒使用，内部预测机制对使用者是不透明的。这使得深度学习模型结果不被使用者信任，增加了落地难度，尤其在医疗、法律等特殊领域。同时，当模型出现效果不好或鲁棒性差等问题时，由于不了解其内部机制，很难对模型进行改进优化。\n",
    "近期，深度学习模型的可解释性被越来越多的人关注。但模型的可解释性评估还不够完善，本基线提供了阅读理解任务的评测数据和相关评测指标，旨在评估模型的可解释性。\n",
    "近期百度发布了一款集可信分析和增强于一体的可信AI工具集TrustAI，旨在探索模型预测机制并增强模型效果。本次基线基于TrustAI搭建。\n",
    "## 2、基线运行\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依赖安装\n",
    "安装一些必须的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-30T08:27:19.765433Z",
     "iopub.status.busy": "2022-09-30T08:27:19.764714Z",
     "iopub.status.idle": "2022-09-30T08:30:49.730024Z",
     "shell.execute_reply": "2022-09-30T08:30:49.728537Z",
     "shell.execute_reply.started": "2022-09-30T08:27:19.765392Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlepaddle-gpu==2.2.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/88/06/edee9889fa8aff3eda68ca34528e13b694d6110e7ad678268eb2fa05994f/paddlepaddle_gpu-2.2.2-cp37-cp37m-manylinux1_x86_64.whl (435.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 MB\u001b[0m \u001b[31m634.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (1.16.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (8.2.0)\n",
      "Requirement already satisfied: numpy>=1.13 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (1.19.5)\n",
      "Requirement already satisfied: astor in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (0.8.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (4.4.2)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (2.24.0)\n",
      "Requirement already satisfied: protobuf>=3.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlepaddle-gpu==2.2.2) (3.20.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.2.2) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.2.2) (2019.9.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.2.2) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.20.0->paddlepaddle-gpu==2.2.2) (2.8)\n",
      "Installing collected packages: paddlepaddle-gpu\n",
      "  Attempting uninstall: paddlepaddle-gpu\n",
      "    Found existing installation: paddlepaddle-gpu 2.3.2.post112\n",
      "    Uninstalling paddlepaddle-gpu-2.3.2.post112:\n",
      "      Successfully uninstalled paddlepaddle-gpu-2.3.2.post112\n",
      "Successfully installed paddlepaddle-gpu-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlenlp==2.3.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/91/65/e6f0cc0ff71f8aed3cd84ac8139341a0405488c83adb57eac51cc47a073f/paddlenlp-2.3.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (4.1.0)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/9d/fd8d73feffeaf5ae3289e5e5b4fc8091d128b52bdfe962aff449d4fab9f8/datasets-2.5.1-py3-none-any.whl (431 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.2/431.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (0.70.11.1)\n",
      "Requirement already satisfied: paddle2onnx in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (1.2.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (4.27.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (0.1.96)\n",
      "Requirement already satisfied: paddlefsl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (1.0.0)\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (0.42.1)\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.3.0) (0.4.4)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (4.2.0)\n",
      "Collecting aiohttp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7a/48/7882af39221fee58e33eee6c8e516097e2331334a5937f54fe5b5b285d9e/aiohttp-3.8.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/08/9b5fe7c9e2774bca77dae29d22a446ead804fb8e050f2899ae1f60d73ad1/pyarrow-9.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (1.1.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (2.24.0)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/82/f3/30f7925f22f623ebac35b40d48151578ef7303d897764e1d95323727611b/fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.8/140.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/79/f3/2b3a6dc5986303b3dd1bbbcf482022acb2583c428cd23f0b6d37b1a1a519/responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: dill<0.3.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (0.3.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (21.3)\n",
      "Collecting xxhash\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ef/ac/0eb796cf052c392f1ae586452c89ca66164f7d4f655b039ca3d06e2291af/xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m588.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from datasets>=2.0.0->paddlenlp==2.3.0) (1.19.5)\n",
      "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9f/81/3ae1223f0ff868907e22ffc16e8e1813ae17b8430c3c1777c9d96c4619ac/huggingface_hub-0.10.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/47/bb/849011636c4da2e44f1253cd927cfb20ada4374d8b3a4e425416e84900cc/tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting paddlefsl\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/4a/25d1959a8f1fe5ee400f32fc9fc8b56d4fd6fc25315e23c0171f6e705e2a/paddlefsl-1.1.0-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.3.0) (0.24.2)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/87/fe94898f2d44a93a35d5aa74671ed28094d80753a1113d68b799fab6dc22/aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/3f/1c876ed190e8fcd1a2faef3085427e5465076e28813a2499502633f7eed3/multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d6/c1/8991e7c5385b897b8c020cdaad718c5b087a6626d1d11a23e1ea87e325a7/async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.0) (4.3.0)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/db/51/a507c856293ab05cdc1db77ff4bc1268ddd39f29e7dc4919aa497f0adbec/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3e/b2/cf7e86583f03fafc93c4103f9a03aaf729dcf4dca9cd3012256a48b766ad/frozenlist-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.0/148.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from aiohttp->datasets>=2.0.0->paddlenlp==2.3.0) (22.1.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e8/b6/8d17e169d577ca7678b11cd0d3ceebb0a6089a7f4a2de4b945fe4b1c86db/asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2b/89/36a50cab1be3d5099ec66a41212cf0c11507c343074e97e907a2f5f1a569/yarl-1.8.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m528.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.3.0) (5.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=2.0.0->paddlenlp==2.3.0) (3.0.12)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from packaging->datasets>=2.0.0->paddlenlp==2.3.0) (3.0.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.0) (2.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests>=2.19.0->datasets>=2.0.0->paddlenlp==2.3.0) (1.25.6)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m915.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.0) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.0) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.3.0) (2.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata->datasets>=2.0.0->paddlenlp==2.3.0) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas->datasets>=2.0.0->paddlenlp==2.3.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets>=2.0.0->paddlenlp==2.3.0) (1.16.0)\n",
      "Installing collected packages: xxhash, urllib3, tqdm, pyarrow, multidict, fsspec, frozenlist, charset-normalizer, asynctest, async-timeout, yarl, aiosignal, responses, paddlefsl, huggingface-hub, aiohttp, datasets, paddlenlp\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.6\n",
      "    Uninstalling urllib3-1.25.6:\n",
      "      Successfully uninstalled urllib3-1.25.6\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.27.0\n",
      "    Uninstalling tqdm-4.27.0:\n",
      "      Successfully uninstalled tqdm-4.27.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 2.0.0\n",
      "    Uninstalling pyarrow-2.0.0:\n",
      "      Successfully uninstalled pyarrow-2.0.0\n",
      "  Attempting uninstall: paddlefsl\n",
      "    Found existing installation: paddlefsl 1.0.0\n",
      "    Uninstalling paddlefsl-1.0.0:\n",
      "      Successfully uninstalled paddlefsl-1.0.0\n",
      "  Attempting uninstall: paddlenlp\n",
      "    Found existing installation: paddlenlp 2.1.1\n",
      "    Uninstalling paddlenlp-2.1.1:\n",
      "      Successfully uninstalled paddlenlp-2.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parl 1.4.1 requires pyzmq==18.1.1, but you have pyzmq 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiohttp-3.8.3 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 charset-normalizer-2.1.1 datasets-2.5.1 frozenlist-1.3.1 fsspec-2022.8.2 huggingface-hub-0.10.0 multidict-6.0.2 paddlefsl-1.1.0 paddlenlp-2.3.0 pyarrow-9.0.0 responses-0.18.0 tqdm-4.64.1 urllib3-1.25.11 xxhash-3.0.0 yarl-1.8.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting trustai==0.1.5\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/7d/30695427906c3a820dc44f6f9563f467d34f2df7ee0502df29c7bb224bb6/trustai-0.1.5-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m805.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (2.2.3)\n",
      "Requirement already satisfied: IPython in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (7.34.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (4.64.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from trustai==0.1.5) (0.24.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (2.0.10)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.17.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (56.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (4.4.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.1.6)\n",
      "Requirement already satisfied: backcall in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (5.4.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (2.13.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from IPython->trustai==0.1.5) (4.7.0)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (1.16.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (2.8.2)\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (2019.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->trustai==0.1.5) (3.0.9)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (1.6.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn->trustai==0.1.5) (2.1.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from jedi>=0.16->IPython->trustai==0.1.5) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pexpect>4.3->IPython->trustai==0.1.5) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->trustai==0.1.5) (0.1.7)\n",
      "Installing collected packages: trustai\n",
      "Successfully installed trustai-0.1.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.2.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U paddlepaddle-gpu==2.2.2\n",
    "!pip3 install -U paddlenlp==2.3.0\n",
    "!pip3 install trustai==0.1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "#### 1）模型训练数据\n",
    "我们推荐使用DuReader-robust数据集训练中文相似度计算模型。Paddlenlp框架会自动下载及缓存训练数据集，默认缓存存储路径为\"~/.paddlenlp/datasets\"。如需修改训练数据，请参考『初始化工作』中DATASET_NAME的修改。\n",
    "#### 2）下载预训练模型\n",
    "基线使用了ERNIE-3.0-base预训练模型。Paddlenlp框架自动缓存模型文件，默认缓存存储路径为\"~/.paddlenlp/models\"。如需修改依赖的预训练模型，请在『初始化工作』中修改MODEL_NAME。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化工作\n",
    "初始化工作包括了模型选择及加载、训练数据集选择、模型存储路径设定、抽取证据的长度占原文本长度的比例设定等。可按需更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-30T08:31:38.529700Z",
     "iopub.status.busy": "2022-09-30T08:31:38.528961Z",
     "iopub.status.idle": "2022-09-30T08:31:57.195706Z",
     "shell.execute_reply": "2022-09-30T08:31:57.194533Z",
     "shell.execute_reply.started": "2022-09-30T08:31:38.529636Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-30 16:31:41,451] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh\n",
      "[2022-09-30 16:31:41,455] [    INFO] - Downloading ernie_3.0_base_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams\n",
      "100%|██████████| 452M/452M [00:08<00:00, 53.1MB/s] \n",
      "W0930 16:31:50.475505   210 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.2\n",
      "W0930 16:31:50.479920   210 device_context.cc:465] device: 0, cuDNN Version: 8.2.\n",
      "[2022-09-30 16:31:56,938] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh\n",
      "[2022-09-30 16:31:56,942] [    INFO] - Downloading ernie_3.0_base_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt\n",
      "100%|██████████| 182k/182k [00:00<00:00, 1.67MB/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import ErnieForQuestionAnswering, ErnieTokenizer\n",
    "from mrc_utils import *\n",
    "\n",
    "# Select pre-trained model\n",
    "MODEL_NAME = \"ernie-3.0-base-zh\" # choose from [\"ernie-1.0\", \"ernie-1.0-base-zh\", \"ernie-1.0-large-zh-cw\", \"ernie-2.0-base-zh\", \"ernie-2.0-large-zh\", \"ernie-3.0-xbase-zh\", \"ernie-3.0-base-zh\", \"ernie-3.0-medium-zh\", \"ernie-3.0-mini-zh\", \"ernie-3.0-micro-zh\", \"ernie-3.0-nano-zh\"]\n",
    "# Select dataset for model training\n",
    "DATASET_NAME = 'dureader_robust'\n",
    "# Set the path to save the trained model\n",
    "MODEL_SAVE_PATH = f'save_model/{DATASET_NAME}-{MODEL_NAME}/1'\n",
    "# Set the rationale length ratio which determines the length of the extracted rationales.\n",
    "RATIONALE_RATIO = 0.096 # 0.096 for Chinese dataset, 0.102 for English dataset\n",
    "\n",
    "# Init model and tokenizer\n",
    "model = ErnieForQuestionAnswering.from_pretrained(MODEL_NAME, num_classes=2)\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "这里以ERNIE-3.0为例训练一个阅读理解模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-30T08:35:19.630307Z",
     "iopub.status.busy": "2022-09-30T08:35:19.629655Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20038/20038 [00:07<00:00, 2571.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Starts:\n",
      "global step 100, epoch: 1, batch: 100, loss: 5.97629\n",
      "global step 200, epoch: 1, batch: 200, loss: 5.95978\n",
      "global step 300, epoch: 1, batch: 300, loss: 5.23865\n",
      "global step 400, epoch: 1, batch: 400, loss: 2.96501\n",
      "global step 500, epoch: 1, batch: 500, loss: 2.71297\n",
      "global step 600, epoch: 1, batch: 600, loss: 2.61423\n",
      "global step 700, epoch: 1, batch: 700, loss: 1.07601\n",
      "global step 800, epoch: 1, batch: 800, loss: 1.71146\n",
      "global step 900, epoch: 1, batch: 900, loss: 1.81692\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 1.86749\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 2.20435\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 1.16324\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 1.85572\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.86686\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 1.44837\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 1.27209\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.79163\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 1.80296\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 1.28911\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 1.13118\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 1.26970\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 1.62544\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 1.30133\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 1.28932\n",
      "global step 2500, epoch: 1, batch: 2500, loss: 0.90954\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 2.18283\n",
      "Processing example: 1000\n",
      "time per 1000: 9.354440212249756\n",
      "F1 on eval dataset: 79.99382541938324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-30 16:43:42,801] [    INFO] - tokenizer config file saved in save_model/dureader_robust-ernie-3.0-base-zh/1/tokenizer_config.json\n",
      "[2022-09-30 16:43:42,804] [    INFO] - Special tokens file saved in save_model/dureader_robust-ernie-3.0-base-zh/1/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2700, epoch: 2, batch: 9, loss: 0.41021\n",
      "global step 2800, epoch: 2, batch: 109, loss: 2.12763\n",
      "global step 2900, epoch: 2, batch: 209, loss: 1.52163\n",
      "global step 3000, epoch: 2, batch: 309, loss: 1.27877\n",
      "global step 3100, epoch: 2, batch: 409, loss: 1.52239\n",
      "global step 3200, epoch: 2, batch: 509, loss: 0.99942\n",
      "global step 3300, epoch: 2, batch: 609, loss: 1.02544\n",
      "global step 3400, epoch: 2, batch: 709, loss: 0.36325\n",
      "global step 3500, epoch: 2, batch: 809, loss: 0.75883\n",
      "global step 3600, epoch: 2, batch: 909, loss: 0.58391\n",
      "global step 3700, epoch: 2, batch: 1009, loss: 0.80946\n",
      "global step 3800, epoch: 2, batch: 1109, loss: 0.19867\n",
      "global step 3900, epoch: 2, batch: 1209, loss: 0.76185\n",
      "global step 4000, epoch: 2, batch: 1309, loss: 2.78727\n",
      "global step 4100, epoch: 2, batch: 1409, loss: 0.70396\n",
      "global step 4200, epoch: 2, batch: 1509, loss: 0.92491\n",
      "global step 4300, epoch: 2, batch: 1609, loss: 1.33201\n",
      "global step 4400, epoch: 2, batch: 1709, loss: 1.89055\n",
      "global step 4500, epoch: 2, batch: 1809, loss: 1.40114\n",
      "global step 4600, epoch: 2, batch: 1909, loss: 2.35560\n",
      "global step 4700, epoch: 2, batch: 2009, loss: 0.72082\n",
      "global step 4800, epoch: 2, batch: 2109, loss: 1.11281\n",
      "global step 4900, epoch: 2, batch: 2209, loss: 0.28250\n",
      "global step 5000, epoch: 2, batch: 2309, loss: 0.90547\n",
      "global step 5100, epoch: 2, batch: 2409, loss: 1.19537\n",
      "global step 5200, epoch: 2, batch: 2509, loss: 1.16128\n",
      "global step 5300, epoch: 2, batch: 2609, loss: 1.33671\n",
      "Processing example: 1000\n",
      "time per 1000: 9.28371000289917\n",
      "F1 on eval dataset: 80.56151112628585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-30 16:51:24,621] [    INFO] - tokenizer config file saved in save_model/dureader_robust-ernie-3.0-base-zh/1/tokenizer_config.json\n",
      "[2022-09-30 16:51:24,693] [    INFO] - Special tokens file saved in save_model/dureader_robust-ernie-3.0-base-zh/1/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 5400, epoch: 3, batch: 18, loss: 0.31900\n",
      "global step 5500, epoch: 3, batch: 118, loss: 0.88382\n",
      "global step 5600, epoch: 3, batch: 218, loss: 0.63239\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "# Hyperparameters\n",
    "batch_size = 6\n",
    "max_seq_length = 512\n",
    "epochs = 3  #3\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "doc_stride = 512\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Load dataset\n",
    "train_ds, dev_ds, test_ds = load_dataset(DATASET_NAME, splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# Start training\n",
    "training_mrc_model(model, \n",
    "                tokenizer,\n",
    "                train_ds, \n",
    "                dev_ds,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                learning_rate=learning_rate,\n",
    "                warmup_proportion=warmup_proportion,\n",
    "                max_seq_length=max_seq_length,\n",
    "                doc_stride=doc_stride, \n",
    "                weight_decay=weight_decay,\n",
    "                save_dir=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要度分数获取\n",
    "该步为输入中每个词赋一个重要度分数，表示该词对预测的影响度。重要度分数获取共分三步。\n",
    "#### 1）加载模型和评测数据集\n",
    "更改模型以及评估数据的存储路径（MODEL_PATH和DATA_PATH），完成模型和评测数据集的加载。赛段一数据量为1855条，赛段二数据量为4366条，请确认评测数据集完整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T02:41:14.965142Z",
     "iopub.status.busy": "2022-09-05T02:41:14.964252Z",
     "iopub.status.idle": "2022-09-05T02:41:16.403763Z",
     "shell.execute_reply": "2022-09-05T02:41:16.402960Z",
     "shell.execute_reply.started": "2022-09-05T02:41:14.965081Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data: 1855\n"
     ]
    }
   ],
   "source": [
    "from utils import load_data\n",
    "from functools import partial\n",
    "\n",
    "# Correct MODEL_PATH and DATA_PATH before executing\n",
    "MODEL_PATH = MODEL_SAVE_PATH + '/model_state.pdparams'\n",
    "DATA_PATH = 'mrc_interpretation.txt'\n",
    "\n",
    "# Load the trained parameters\n",
    "state_dict = paddle.load(MODEL_PATH)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# Load test data\n",
    "data_ds = DuReader().read(DATA_PATH)\n",
    "data = load_data(DATA_PATH)\n",
    "print(\"Num of data:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2）数据预处理\n",
    "\n",
    "a) 输入格式化：将输入的两个文本组织成模型预测所需格式，如对于Ernie3.0-base模型，其输入形式为[CLS]question[SEP]context[SEP]\n",
    "\n",
    "b) 分词位置索引：计算每个分词结果对应的原文位置索引，这里的分词包括模型分词和标准分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mrc_utils import *\n",
    "# Hyperparameters\n",
    "batch_size = 12\n",
    "max_seq_length = 512\n",
    "epochs = 3  #3\n",
    "warmup_proportion = 0.1\n",
    "weight_decay = 0.01\n",
    "doc_stride = 512\n",
    "\n",
    "# Prepare dataloader\n",
    "test_trans_func = partial(prepare_validation_features, \n",
    "                            max_seq_length=max_seq_length, \n",
    "                            doc_stride=doc_stride,\n",
    "                            tokenizer=tokenizer)\n",
    "                            \n",
    "data_ds.map(test_trans_func, batched=True, num_workers=4)\n",
    "test_batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "        data_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_batchify_fn = lambda samples, fn=Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\n",
    "}): fn(samples)\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=data_ds,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=test_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "# Get offset maps which will be used for score alignment\n",
    "contexts, standard_split, ori_offset_maps, standard_split_offset_maps = pre_process(data, data_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3）重要度分数获取\n",
    "我们提供attention和IG两种解释方法，可根据实际实验结果选取最有效的一种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a） Attention-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T02:41:28.548486Z",
     "iopub.status.busy": "2022-09-05T02:41:28.547747Z",
     "iopub.status.idle": "2022-09-05T02:41:39.942827Z",
     "shell.execute_reply": "2022-09-05T02:41:39.941908Z",
     "shell.execute_reply.started": "2022-09-05T02:41:28.548449Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level import AttentionInterpreter\n",
    "from utils import create_dataloader_from_scratch\n",
    "import paddle\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Init an attention interpreter and get the importance scores\n",
    "att = AttentionInterpreter(model, device=\"gpu\", predict_fn=attention_predict_fn)\n",
    "\n",
    "# Use attention interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in test_data_loader:\n",
    "    if interp_results:\n",
    "        interp_results += att(batch)\n",
    "    else:\n",
    "        interp_results = att(batch)\n",
    "\n",
    "# Trim the output to get scores only for context\n",
    "interp_results = trim_output(interp_results, data_ds, tokenizer)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = att.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b）IG-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level import IntGradInterpreter\n",
    "from utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "IG_STEP = 100\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Init an IG interpreter\n",
    "ig = IntGradInterpreter(model, predict_fn=IG_predict_fn, device=\"gpu\")\n",
    "\n",
    "# Use IG interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in test_data_loader:\n",
    "    if interp_results:\n",
    "        interp_results += ig(batch, steps=IG_STEP)\n",
    "    else:\n",
    "        interp_results = ig(batch, steps=IG_STEP)\n",
    "\n",
    "# trim the output to get scores only for context\n",
    "interp_results = trim_output(interp_results, data_ds, tokenizer)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = ig.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用于评估的数据\n",
    "评估文件格式要求是3列数据：编号\\t预测答案\\t证据，我们提供了脚本将模型输出结果转成评估所需格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-05T03:37:41.743891Z",
     "iopub.status.busy": "2022-09-05T03:37:41.742925Z",
     "iopub.status.idle": "2022-09-05T03:37:41.913516Z",
     "shell.execute_reply": "2022-09-05T03:37:41.912478Z",
     "shell.execute_reply.started": "2022-09-05T03:37:41.743853Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Re-sort the token index according to their importance scores\n",
    "def resort(index_array, importance_score):\n",
    "    res = sorted([[idx, importance_score[idx]] for idx in index_array], key=lambda x:x[1], reverse=True)\n",
    "    res = [n[0] for n in res]\n",
    "    return res\n",
    "\n",
    "# Post-prepare the result data so that it can be used for the evaluation directly\n",
    "def prepare_eval_data(data, results, paddle_model):\n",
    "    res = {}\n",
    "    idx = 0\n",
    "    for data_id, inter_res in zip(data, results):\n",
    "        \n",
    "        # Split importance score vectors for query and title from inter_res.word_attributions\n",
    "        importance_score = np.array(inter_res.word_attributions[1:-1])\n",
    "        # Extract topK importance scores\n",
    "        topk = math.ceil(len(data[data_id]['sent_token'])*RATIONALE_RATIO)\n",
    "        \n",
    "        eval_data = {}        \n",
    "        eval_data['id'] = data_id\n",
    "        label = list(inter_res.pred_label)\n",
    "        if int(label[0])>=int(label[1])+1:\n",
    "            eval_data['pred_label'] = ''\n",
    "        else:\n",
    "            eval_data['pred_label'] = ''.join(tokenizer.convert_ids_to_tokens(data_ds[idx]['input_ids'][int(label[0]):int(label[1])+1]))\n",
    "        # Find the token index of the topK importance scores\n",
    "        eval_data['rationale'] = np.argpartition(importance_score, -topk)[-topk:]\n",
    "        # Re-sort the token index according to their importance scores\n",
    "        eval_data['rationale'] = resort(eval_data['rationale'], importance_score)\n",
    "\n",
    "        res[data_id] = eval_data\n",
    "        idx += 1\n",
    "    return res\n",
    "\n",
    "# Generate results for evaluation\n",
    "predicts = prepare_eval_data(data, align_res, model)\n",
    "out_file = open('./mrc_rationale.txt', 'w')\n",
    "for key in predicts:\n",
    "    out_file.write(str(predicts[key]['id'])+'\\t'+ str(predicts[key]['pred_label'])+'\\t')\n",
    "    for idx in predicts[key]['rationale'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    out_file.write(str(predicts[key]['rationale'][-1])+'\\n')\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
